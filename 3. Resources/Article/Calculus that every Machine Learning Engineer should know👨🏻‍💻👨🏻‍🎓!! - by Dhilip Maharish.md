## Calculus that every Machine Learning Engineer should knowüë®üèª‚Äçüíªüë®üèª‚Äçüéì!!

**Calculus is like a Fuel which power in optimize the Machine Learning Model!!**

![**Source: Istocks Images**](https://cdn-images-1.medium.com/max/2048/1*JdX22I3G2syPpG-bNMTZMA.jpeg)

In the world of Data Science, there is an important concept that make Machine Learning Model training with **efficient** and **optimized** way that will provide with high accuracy in making prediction or guess correct output by model.

Yes, it is none other than **Calculus. **The basics foundational concepts that build the process in training every machine learning model in optimized way.

In this article I am going to explain how **Calculus** contributes to Machine Learning in Data Science how they help in training for machine learning model.

so, lets deep dive into the essential **Calculus** that makes Data Science so powerful.

First, we can get clear on this **what data science is?**

The title itself explains you, taking Data and applying **scientifical** concepts like **statistics**, **probability** and **calculus** to derive the meaningful insights out of it.
>  ***Data Science is understanding Past information and predicting future information.***

Examples:

Data science helps us predict the future, like a weather forecast telling us if it will rain tomorrow. It is not a magic it uses number and machine learning. It‚Äôs about finding the truth in data. It helps us answer questions and solve problems.

Now we can get into **What is Calculus and why we need in Machine Learning how it contributes in it?**

Calculus is the branch of Mathematics that calculate the **rate of change** of things. It primarily consists of two types they are **Differential** and **Inferential** Calculus.
>  ***Differential calculus deals with rates of change and slopes of curves. Integral calculus involves total quantity over an interval.***

![**Source: Dhilip Maharish- Author**](https://cdn-images-1.medium.com/max/2000/1*DGqe4Z5e5NcBpuaJR3IIcQ.png)

But we often use the **differential **calculus in Machine Learning to optimize in training. It helps in some of the machine Learning model like **Linear **and **Logistic Regression**, **Support vector machines**, **Principal component analysis** and **Neural network**.

why we need calculus in Machine Learning explained with an example.

Example:

When building a Machine Learning Model to train a robot to recognize whether a fruit is an apple or not, the model needs to learn from its mistakes to make better decisions. To achieve this, the model uses calculus to guide its learning process. Calculus helps the model make precise adjustments based on the shape and size of the fruit it encounters. With each mistake it makes, the model uses calculus to understand how it should adapt its decision-making process. This iterative learning, driven by calculus, enables the model to become increasingly accurate in distinguishing apples from other fruits.

![**Source: Google Images**](https://cdn-images-1.medium.com/max/2000/0*bUiQW-6UZaikA4RG.jpg)

Let see what are the calculus topics which covers in Machine Learning.

‚û°Ô∏è**Minima**- In calculus, particularly in the context of functions and optimization, ‚Äú**global minima**‚Äù and ‚Äú**local minima**‚Äù refer to specific points on a curve or surface of a mathematical function.

1. **Local Minima**- A ‚Äú**local minimum**‚Äù of a function is a point where the function has a **lower** value than at nearby points but not necessarily the lowest value over the entire domain.

Mathematically, a local minimum point ***xl***‚Äã is defined as follows:

**f(xl) < f(x)** for all ***x*** in a neighborhood of *xl*‚Äã.

**2. Global Minima**- The ‚Äú**global minimum**‚Äù of a function is the lowest value of that function over its entire domain. In other words, it is the absolute lowest point on the curve or surface.

Mathematically, a global minimum point ***xg*‚Äã **is defined as follows:

**f(xg) < f(x)** for all x in the domain of the function.

‚û°Ô∏è**Maxima**- In calculus, particularly in the context of functions and optimization, ‚Äú**global maxima**‚Äù and ‚Äú**local maxima**‚Äù refer to specific points on a curve or surface of a mathematical function.

 1. **Local Maxima**- The ‚Äú**local maximum**‚Äù of a function is a point where the function has a **higher **value than at nearby points but not necessarily the highest value over the entire domain.

Mathematically, a local maximum point ***xl***‚Äã is defined as follows:

**f(xl) > f(*x*)** for all ***x*** in a neighborhood of *xl*‚Äã.

2**. Global Maxima**- The ‚Äú**global maximum**‚Äù of a function is the **highest** value of that function over its entire domain. In other words, it is the absolute highest point on the curve or surface.

Mathematically, a global maximum point ***xg***‚Äã is defined as follows:

***f(xg) > f*(*x*)** for all ***x*** in the domain of the function.

![**Source: Dhilip Maharish- Author**](https://cdn-images-1.medium.com/max/2000/1*gpR-e8HtX3E6tdIKiYYt9g.png)

In the context of machine learning in optimization, ‚Äú**minima**‚Äù and ‚Äú**maxima**‚Äù refer to the minimum and maximum values of a function, respectively. These concepts are important when dealing with **loss** or **cost** functions, which are used to measure how well a machine learning model is performing.

‚û°Ô∏è**Gradient Descent**- Gradient descent is an **optimization algorithm** used to **minimize** (or sometimes maximize) a function by iteratively adjusting its **input parameters**. Gradient descent uses the principles of calculus, particularly **differential calculus**, to guide the optimization process. It is commonly employed in machine learning, deep learning, and various other fields were finding the optimal solution is essential.

![**Source: Dhilip Maharish- Author**](https://cdn-images-1.medium.com/max/2256/1*QAnRXcdPFumU7CyjM7jLHg.png)

‚û°Ô∏è**Stochastic Gradient Descent**- Stochastic Gradient Descent (SGD) is a variant of the gradient descent optimization algorithm used in machine learning and deep learning. It‚Äôs designed to be more computationally efficient than traditional gradient descent, especially when dealing with large datasets.

‚û°Ô∏è**Chain Rule**- The chain rule is a fundamental concept in calculus that allows you to find the **derivative of a composite** function. In other words, it tells you how to find the **derivative of a function** that is made up of two or more functions nested inside each other. The chain rule in calculus plays a crucial role in machine learning, such as **gradient computation**, **backpropagation** in **neural networks**, and understanding the relationships between model parameters and the loss function.

![**Source: Dhilip Maharish- Author**](https://cdn-images-1.medium.com/max/2624/1*x8viEL8I8CPmz6OYCWbixA.png)

‚û°Ô∏è**Backpropagation**- Backpropagation, short for ‚Äú**backward propagation of errors,**‚Äù is a fundamental algorithm used in training artificial neural networks, including deep learning models. It is a key component of the training process for these networks and enables them to learn from data by adjusting their internal parameters (**weights** and **biases**) to minimize the error or loss between predicted and actual outputs.

![**Source- Dhilip Maharish- Author**](https://cdn-images-1.medium.com/max/2000/1*5HXN_4BUDDgaROuZE44mgg.png)

‚û°Ô∏è**Vanishing Gradient**- The term ‚Äú**vanishing gradient**‚Äù refers to a problem that can occur during the training of deep neural networks. This problem occurs when the **gradients **(derivatives of the loss with respect to the network‚Äôs parameters) become **extremely small** as they are backpropagated through the layers of the network during training.

![**Source- Dhilip Maharish- Author**](https://cdn-images-1.medium.com/max/2000/1*W9tDnsLzBF_PSXCRDLODNQ.png)

‚û°Ô∏è**Exploring Gradient**- The term ‚Äú**exploding gradient**‚Äù is the opposite problem of the vanishing gradient and refers to a situation during the training of deep neural networks where the **gradients** (derivatives of the loss with respect to the network‚Äôs parameters) become **extremely large **as they are backpropagated through the layers of the network.

How it is contributes to Machine Learning are listed below,

‚úÖ**Calculus in training the machine learning model**

When training models like linear regression, calculus is used to compute **gradients** and **partial derivatives**. These gradients guide the model‚Äôs parameter updates during training, ensuring that it converges to the optimal solution.

‚úÖ**Calculus in training Deep Neural Network**

In neural networks, the **backpropagation** algorithm relies on calculus to compute **gradients of the loss function **with respect to the network‚Äôs **weights** and **biases**. This information is crucial for adjusting the **network‚Äôs parameters** during training, allowing it to learn and make better predictions.

‚úÖ**Calculus in optimizing the machine learning model**

Various optimization algorithms used in machine learning, such as **Adam** and **RMSpro**p, use calculus to adapt **learning rates** and update model parameters efficiently during training.

‚úÖ**Calculus in regularizing the model**

Techniques like **L1 **and **L2 **regularization involve calculus to add **penalty** terms to the **loss function**. These penalty terms help prevent **overfitting** by encouraging simpler models, and their calculus-based formulation ensures proper balance between fitting the data and preventing overcomplexity.

## Conclusion

I end up an article with final conclusion that Calculus is the essential tool for **Machine Learning Engineers**. It acts like a strong foundation in Machine Learning model. This help in making Artificial Intelligence make so powerful. Although Calculus is seeming to be more complex but understanding with example and hands-on practice on training model will become easier and should master it if you want ot entire as the field in Machine Leaning.

If you like the above article kindly provide claps and